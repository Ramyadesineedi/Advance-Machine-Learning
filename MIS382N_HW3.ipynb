{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Nb3QnVJJbF-b"
   },
   "source": [
    "# <p style=\"text-align: center;\">MIS 382N: Advanced Machine Learning</p>\n",
    "# <p style=\"text-align: center;\">Homework 3</p>\n",
    "## <p style=\"text-align: center;\">Total points: 85</p>\n",
    "## <p style=\"text-align: center;\">Due: Monday, **Oct 18th** submitted via Canvas by 11:59 pm</p>\n",
    "\n",
    "Your homework should be written in a **Jupyter notebook**. Please make sure your code runs and the graphics (and anything else) are displayed in your notebook before submitting (%matplotlib inline). \n",
    "\n",
    "**Note: Notebooks MUST have the images embedded in them. There will be no regrades if attached images do not render in the notebook. Please re download from canvas after submission and make sure all attached images render without errors. (Hint: Image module from IPython.display)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wkXOyWSCbMlw"
   },
   "source": [
    "**This can be an individual assignment or group of 2. If you choose to do it as a group, please specify who you are working with (name and EID), then only one student should submit the homework. Put your name and eid here.**\n",
    "\n",
    "Name:\n",
    "\n",
    "EID:\n",
    "\n",
    "Name:\n",
    "\n",
    "EID:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LgyLF87bPkA"
   },
   "source": [
    "# Question 1: Stochastic gradient descent II (20 pts)\n",
    "\n",
    "Write an SGD solution in Python to the non-linear model without using any other library except for those provided in the code template below. \n",
    "\n",
    "$$ y = w_0 + w_1x_1 + w_2x_1x_2 + w_3x_1^2x_2 + w_4x_1x_2^2 + w_5x_2^3$$ \n",
    "\n",
    "The solution class template is given. The `init()` function of the class takes as input the `learning_rate`, `regularization_constant` and `number_of_epochs`. The `fit()` method must take as input `X`, `y` and `update_rule` which can be `'sgd_momentum'` or `'RMSprop'`. The `predict()` method takes an `X` value (optionally, an array of values). \n",
    "\n",
    "Use your new gradient descent regression to train your model and predict the data given in 'SGD_samples.csv', for 30 epochs, using learning rates: `[0.0001, 0.001, 0.01, 0.05]` and regularization (ridge regression) constants: `[0, 0.01, 0.1]`. **(8 points)** \n",
    "\n",
    "Plot MSE and the $w$ parameters as a function of epoch (for 30 epochs) for the best 2 combinations of `learning_rate` and `regularization_constant` for SGD-Momentum, RMSprop, i.e., for each combination, you should have one plot for MSE vs Epoch and another for the parameters(weights) vs Epoch using respectively SGD-Momentum and RMSprop, hence in total 8 plots. Report the `learning_rate`, `regularization_constant` and MSE at the end of 30 epochs for the two best combinations for SGD-Momentum and RMSprop respectively. **($4\\times 2 = 8$ pts)**\n",
    "\n",
    "Observe the results, compare the performance of the two learning methods **(4 pts)**. \n",
    "\n",
    "Here is a blog which you can go through to know about RMSprop and Adam - [blog](http://ruder.io/optimizing-gradient-descent/).\n",
    "\n",
    "Following codes are for your reference, please don't change the initialization values of the given parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XVVk-mks4laL"
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "num_of_coefs = 6\n",
    "\n",
    "class LinearRegression:\n",
    "    \n",
    "    def __init__(self, learning_rate, regularization, n_epoch):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epoch = n_epoch\n",
    "        self.regularization = regularization\n",
    "        self.eps = 10**-8\n",
    "        self.coef = np.zeros(num_of_coefs) # coefficient vector\n",
    "        self.cache = np.zeros(self.coef.shape) # used only for rmsprop\n",
    "        self.gama = 0.9 # used only for rmsprop\n",
    "        self.mu = 0.9 # used in momentum\n",
    "\n",
    "    def rmsprop(self, gradient):\n",
    "        '''\n",
    "        updates self.coef based on gradient using rmsprop\n",
    "        '''\n",
    "    \n",
    "    def sgd_momentum(self, gradient):\n",
    "        '''\n",
    "        updates self.coef based on gradient using Sgd_momentum\n",
    "        '''\n",
    "    \n",
    "    def fit(self, X, y, update_rule='sgd_momentum', plot=False):\n",
    "        '''\n",
    "        Fit the model given X, y. It uses the specified update_rule\n",
    "        and displays a plot of the coefficients vs epochs, and mse vs epochs if plot is set as True. \n",
    "        \n",
    "        -> use get_features to get the features from X\n",
    "        -> for epoch in epochs:\n",
    "            iterate through all x, y.\n",
    "                compute prediction using linearPredict.\n",
    "                compute gradient.\n",
    "                pass this gradient to the corresponding update function and update the coefficients\n",
    "                keep track of mse and coefficients\n",
    "        -> plot if required\n",
    "        \n",
    "        '''\n",
    "\n",
    "    def get_features(self, X):\n",
    "       x = np.zeros((X.shape[0], num_terms))\n",
    "       x[:,0] = 1\n",
    "       x[:,1] = X[:,0]\n",
    "       x[:,2] = X[:,0]*X[:,1]\n",
    "       x[:,3] = (X[:,0]**2)*X[:,1]\n",
    "       x[:,4] = X[:,0] * (X[:,1]**2)\n",
    "       x[:,5] = X[:,1]**3\n",
    "       \n",
    "       return x\n",
    "        \n",
    "    def linearPredict(self, X_features):\n",
    "       '''\n",
    "       returns the dot product of X and self.coef\n",
    "       '''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XE3pcI6O7Jvj"
   },
   "outputs": [],
   "source": [
    "# Only use this code block if you are using Google Colab.\n",
    "# If you are using Jupyter Notebook, please ignore this code block. You can directly upload the file to your Jupyter Notebook file systems.\n",
    "from google.colab import files\n",
    "\n",
    "## It will prompt you to select a local file. Click on “Choose Files” then select and upload the file. \n",
    "## Wait for the file to be 100% uploaded. You should see the name of the file once Colab has uploaded it.\n",
    "uploaded = files.upload()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KMCFUos-5Brh"
   },
   "outputs": [],
   "source": [
    "import random\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv('SGD_samples.csv')\n",
    "X = np.array([data['x1'].values, data['x2'].values]).T\n",
    "y = data['y'].values\n",
    "n_epochs = 30\n",
    "learning_rate = [0.0001, 0.001, 0.01, 0.05]\n",
    "regularization = [0, 0.01, 0.1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0xrLYAi3GtV"
   },
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "H6pUQ7SvbU4G"
   },
   "source": [
    "#Question 2: Tensorflow Playground (15 pts)\n",
    "\n",
    "In this question, you will be playing with [Tensorflow Playground](https://playground.tensorflow.org).\n",
    "\n",
    "Select \"**Classification**\" as the **Problem Type**. Among the four datasets shown in DATA, please select the **top right** dataset. \n",
    "\n",
    "Use the following settings as the DEFAULT settings for all **subquestions**: Learning rate = 0.03, Activation = Tanh, Regularization = None, Ratio of training to test data = 50%, Noise = 0, Batch Size = 30, input as $X_1$ with $X_2$, One hidden layer with two neurons.\n",
    "\n",
    "a) **(4 pts)** Use the DEFAULT setting and run two experiments - one using **Tanh** as the activation function and one using the **Linear** activation function. Report the train, test losses for both at the end of **1000 epochs**. What qualitative difference do you observe in the decision boundaries obtained? What do you think is the reason for this? \n",
    "\n",
    "We will now study the effect of certain variations in the network structure or training process, keeping all other aspects the same as in the DEFAULT setting specified above, with **Tanh** as the activation.\n",
    "\n",
    "b) **(4 pts)** Effect of number of hidden units: Keep other settings the same as in DEFAULT, report the training loss and test loss at the end of 1000 epochs **using 4 neurons and 8 neurons in the hidden layer**. What do you observe in terms of the decision boundary obtained as the number of neurons increases? What do you think is the reason for this? \n",
    "\n",
    "c) **(4 pts)** Effect of Learning rate and number of epochs: Keep other settings the same as in DEFAULT, change the Activation to **ReLU** and use **4 neurons** in the hidden layer. For learning rate 10, 1, 0.1, 0.01 and 0.001, report the train, test losses at the end of **100 epochs**, **1000 epochs** respectively. What do you observe from the change of loss vs learning rate, and the change of loss vs epoch numbers? \n",
    "\n",
    "d) **(3 pts)** Use the DEFAULT setting. Play around with any hyperparameters, network architectures or input features (such as $\\sin(X_1), X_1^2$ etc.), and report the best train and test loss you obtain (test loss should be no greater than 0.06). Attach the screenshot showing your full network, output and the parameters. Briefly justify your results, and comment on what helps/what doesn't help with lowering the loss, etc. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l85_A1P33Nr4"
   },
   "source": [
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rYljtggnkSzl"
   },
   "source": [
    "# Question 3: Outlier detection using PyOD (30 pts)\n",
    "\n",
    "Oulier detection, or anomaly detection is usually an unsupervised learning task where the objective is to identify suspicious observations in data. It has been widely used in military surveillance for enemy activities to prevent attacks, intrusion detection in cyber security, fraud detection for credit cards, etc.\n",
    "\n",
    "PyOD is a comprehensive and scalable Python library for detecting outlying objects in multivariate data. PyOD includes more than 30 detection algorithms and provides unified APIs which makes it quite handy to use. In this question, you will play with PyOD, explore three different outlier detection algorithms and compare their performances. First let's install PyOD.\n",
    "\n",
    "```\n",
    "# install pyod using pip first\n",
    "!pip install pyod\n",
    "```\n",
    "\n",
    "You can load the data stored in 'Q3_train_dataset.csv' and 'Q3_test_dataset.csv' using the following codes.\n",
    "\n",
    "```\n",
    "import pandas as pd\n",
    "# Load data code goes here\n",
    "train_df = pd.read_csv('Q3_train_dataset.csv')\n",
    "test_df = pd.read_csv('Q3_test_dataset.csv')\n",
    "\n",
    "X_train = train_df[['X_train_0', 'X_train_1', 'X_train_2', 'X_train_3', 'X_train_4']].to_numpy()\n",
    "y_train = train_df[['y_train']].to_numpy()\n",
    "X_test = test_df[['X_test_0', 'X_test_1', 'X_test_2', 'X_test_3', 'X_test_4']].to_numpy()\n",
    "y_test = test_df[['y_test']].to_numpy()\n",
    "```\n",
    "\n",
    " `X_train` and `X_test` contain the features, with the dimension of 5. `y_train` and `y_test` store the outlier labels, 0 means normal data, 1 means outlier data. \n",
    " \n",
    "a) **(5 pts)** **Fit `X_train` to a linear outlier detection model Minimum Covariance Determinant (MCD) using PyOD**, this [page](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.mcd) will provide some functions you may need. \n",
    "\n",
    "```\n",
    "from pyod.models.mcd import MCD\n",
    "clf = MCD() # initialize MCD class using the default parameters\n",
    "\n",
    "# YOUR CODE SHOULD COME HERE, FIT THE MODEL USING X_TRAIN\n",
    "```\n",
    "**Use the fitted model to predict the outlier labels of `X_test`. Compute the raw outlier scores on `X_test` using `decision_function()`.**\n",
    "\n",
    "**Run PyOD's `evaluate_print()` function using the test set ground truth outlier labels and the raw outlier scores predicted by the model, to compute the ROC and Precision@n results .** \n",
    "\n",
    "```\n",
    "from pyod.utils.data import evaluate_print\n",
    "```\n",
    "\n",
    "b) **(5 pts)** `X_train` and `X_test` are 5-dimension features, which makes it impossible to visualize them in Euclidean plane. But we can use Principal Component Analysis (PCA) to reduce the dimensions of `X_train` and `X_test` to 2, and then plot them. You may want to use `fit_and_transform()` function.\n",
    "\n",
    "\n",
    "```\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit pca to X_train and X_test and transform \n",
    "train_principalComponents = # IMPLEMENT\n",
    "test_principalComponents = # IMPLEMENT\n",
    "```\n",
    "\n",
    "After reducing the dimension to 2, now you can visualize the outliers using PyOD's `visualize()` function. Please plot the visualization. You may find [this](https://pyod.readthedocs.io/en/latest/pyod.utils.html#module-pyod.utils.example) useful on how to use `visualize()` .\n",
    "\n",
    "```\n",
    "from pyod.utils.example import visualize\n",
    "```\n",
    "\n",
    "Now you should be able to observe the ground truth outliers and the outliers predicted by the model.\n",
    "\n",
    "\n",
    "c) **(20 pts)** Apply the same process as in (a) and (b) to the following two models, and visualize the outlier results. Please compare the performance of the three models in terms of their ROC, Precision@n, and what you observe from the three visualizations.\n",
    "\n",
    "*   [Proximity-Based model - Clustering Based Local Outlier Factor (CBLOF)](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.cblof)\n",
    "*   [Probabilistic model - Copula-based Outlier Detection (COPOD)](https://pyod.readthedocs.io/en/latest/pyod.models.html#module-pyod.models.copod)\n",
    "\n",
    "```\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.copod import COPOD\n",
    "```\n",
    "\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BULJ0M0sL1gf"
   },
   "outputs": [],
   "source": [
    "# install pyod using pip first\n",
    "!pip install pyod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qEY5NClZL9a8"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Load data code goes here\n",
    "train_df = pd.read_csv('Q3_train_dataset.csv')\n",
    "test_df = pd.read_csv('Q3_test_dataset.csv')\n",
    "\n",
    "X_train = train_df[['X_train_0', 'X_train_1', 'X_train_2', 'X_train_3', 'X_train_4']].to_numpy()\n",
    "y_train = train_df[['y_train']].to_numpy()\n",
    "X_test = test_df[['X_test_0', 'X_test_1', 'X_test_2', 'X_test_3', 'X_test_4']].to_numpy()\n",
    "y_test = test_df[['y_test']].to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oRPwxrFQL_Y3"
   },
   "outputs": [],
   "source": [
    "# (a)\n",
    "from pyod.models.mcd import MCD\n",
    "clf = MCD() # initialize MCD class using the default parameters\n",
    "\n",
    "# fit the model using X_train\n",
    "\n",
    "# YOUR CODE SHOULD COME HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pwFvjZQ5MBE_"
   },
   "outputs": [],
   "source": [
    "from pyod.utils.data import evaluate_print\n",
    "# predict the outlier labels of X_test using the trained model, compute the raw outlier scores on X_test using decision_function()\n",
    "# then use evaluate_print() to print out the evaluation results\n",
    "\n",
    "# YOUR CODE SHOULD COME HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "RBIhWWz-MC1C"
   },
   "outputs": [],
   "source": [
    "# (b)\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pca = PCA(n_components=2)\n",
    "\n",
    "# Fit pca to X_train and X_test and transform \n",
    "train_principalComponents = # YOUR CODE SHOULD COME HERE\n",
    "test_principalComponents = # YOUR CODE SHOULD COME HERE\n",
    "\n",
    "from pyod.utils.example import visualize\n",
    "# Visualize the ground truth outliers and predicted outliers using visualize()\n",
    "\n",
    "# YOUR CODE SHOULD COME HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dSs_Kej_MIN5"
   },
   "outputs": [],
   "source": [
    "# (c)\n",
    "from pyod.models.cblof import CBLOF\n",
    "from pyod.models.copod import COPOD\n",
    "\n",
    "# YOUR CODE SHOULD COME HERE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "98Ipdr7rbdal"
   },
   "source": [
    "#Question 4: PCA Conceptual questions (5 pts)\n",
    "Explain the principle of Principal Component Analysis algorithm, especially why we can select the best projection bases based on the covariance matrix of data from the perspective of optimization?\n",
    "\n",
    "# Answer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a7LS1E520XPL"
   },
   "source": [
    "\n",
    "\n",
    "# Question 5: Pre-processing and sampling (15 pts)\n",
    "\n",
    "The following dataset contains House prices describing the sales of individual residential property in Ames, Iowa data with explanatory variables describing almost every aspect of residential homes and dependent variable being SalePrice. Here, some cells of most columns in the dataset contain NaN values.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HzXBJJihpKum"
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import scipy.stats as stats\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "np.random.seed(100)\n",
    "\n",
    "data = pd.read_csv(\"sales_data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SZpAfufgqDnc"
   },
   "source": [
    "\n",
    "a) **(2 pts)** Print the number of NaN values in each column.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4ZJ-OvcIO9bW"
   },
   "outputs": [],
   "source": [
    "#"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2qX1tYc9PTeq"
   },
   "source": [
    "b) **(3 pts)** Create a copy of `data`, and name it `data_dm`, then create a new column in `data_dm` named `binned_yr_built` and apply binning to the column `yr_built`. Use `pandas.cut()` and modify its paramter list as below:\n",
    "\n",
    "```\n",
    "bins=[1900, 1920, 1940, 1960, 1980, 2000, 2020]\n",
    "labels=['1900-1920', '1920-1940', '1940-1960', '1960-1980', '1980-2000', '2000-2020']\n",
    "include_lowest=True\n",
    "```\n",
    "Next, perform one-hot encoding on this new column."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "f5JLgp3ePXaH"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Fca0nB-KP5X8"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DZG-thPCPe8J"
   },
   "source": [
    "\n",
    "c) **(2 pts)** Drop the columns which have more than 65 percentage of missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4wwgOq-_PlXV"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1F_Tj5rtPSLw"
   },
   "source": [
    "\n",
    "d) **(3 pts)** Take a sample of 800 rows at random and compute its mean, compare this value with the population mean.\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LqijYNMkPS-a"
   },
   "outputs": [],
   "source": [
    "np.random.seed(6)\n",
    "sample_ages = np.random.choice(a= data['SalePrice'], size=800)\n",
    "#Sample mean\n",
    "\n",
    "#Population mean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2FfEuB-hQ9a5"
   },
   "source": [
    "e) **(2pts)** Calculate the 95% confidence intervals for SalePrice with a sample size of 100. \n",
    "\n",
    "**(3pts)** Calculate the 95% confidence intervals for 100 different trials with a sample size of 500. Plot the confidence intervals and interpret how it captures the population mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nrXDpIsgUhPs"
   },
   "outputs": [],
   "source": [
    "np.random.seed(10)\n",
    "sample_size = 100\n",
    "sample = np.random.choice(a= data_dm['SalePrice'],\n",
    "                          size = sample_size)\n",
    "sample_mean = sample.mean()\n",
    "\n",
    "#Get the critical Z value\n",
    "\n",
    "\n",
    "#Get population standard deviation\n",
    "\n",
    "\n",
    "#margin of error\n",
    "margin_of_error = z_critical * (pop_stdev/math.sqrt(sample_size)) \n",
    "\n",
    "#confidence interval\n",
    "confidence_interval = (sample_mean - margin_of_error,\n",
    "                       sample_mean + margin_of_error)  \n",
    "\n",
    "#Print confidence interval and true mean value\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vpX5NGr0VhyM"
   },
   "outputs": [],
   "source": [
    "np.random.seed(12)\n",
    "\n",
    "sample_size = 500\n",
    "\n",
    "intervals = []\n",
    "sample_means = []\n",
    "\n",
    "for sample in range(100):\n",
    "    sample = np.random.choice(a= data_dm['SalePrice'], size = sample_size)\n",
    "    sample_mean = sample.mean()\n",
    "    sample_means.append(sample_mean)\n",
    "    # compute z critical value\n",
    "\n",
    "    # compute population standard deviation     \n",
    "  \n",
    "    margin_of_error = z_critical * (pop_stdev/math.sqrt(sample_size))\n",
    "    confidence_interval = (sample_mean - margin_of_error,\n",
    "                           sample_mean + margin_of_error)  \n",
    "    \n",
    "    intervals.append(confidence_interval)\n",
    "    \n",
    "plt.figure(figsize=(13, 9))\n",
    "\n",
    "plt.errorbar(x=np.arange(0.1, 100, 1), \n",
    "             y=sample_means, \n",
    "             yerr=[(top-bot)/2 for top,bot in intervals],\n",
    "             fmt='o')\n",
    "\n",
    "plt.hlines(xmin=0, xmax=25,\n",
    "           y=data['SalePrice'].mean(), \n",
    "           linewidth=2.0,\n",
    "           color=\"red\")\n",
    "plt.title('Confidence Intervals for 100 Trials', fontsize = 20)\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "MIS382N_HW3.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
